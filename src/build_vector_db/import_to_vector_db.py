"""
æ™ºèƒ½å¢é‡å¯¼å…¥å‘é‡æ•°æ®åº“è„šæœ¬
åŠŸèƒ½ï¼šæ‰«æç›®å½•ï¼Œæ™ºèƒ½æ£€æµ‹æ–‡ä»¶å˜æ›´ï¼Œé¿å…é‡å¤å¯¼å…¥
"""

import json
import os
import hashlib
import shutil
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Set, Tuple
import sys

# ==================== LangChain å¯¼å…¥ ====================
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma

# ==================== æœ¬åœ°å¯¼å…¥ ====================
try:
    from embeddings import Text2VecEmbeddings
except ImportError:
    print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°embeddingsæ¨¡å—")
    sys.exit(1)


class SmartDocumentImporter:
    """
    æ™ºèƒ½æ–‡æ¡£å¯¼å…¥å™¨
    æ”¯æŒå¢é‡å¯¼å…¥å’Œé‡å¤æ£€æµ‹
    """
    
    def __init__(self, model_path: str = "/root/models/text2vec-large-chinese"):
        """
        åˆå§‹åŒ–å¯¼å…¥å™¨
        """
        print(f"ğŸ§  åˆå§‹åŒ–æ™ºèƒ½æ–‡æ¡£å¯¼å…¥å™¨...")
        
        # åµŒå…¥æ¨¡å‹
        self.embeddings = Text2VecEmbeddings(model_path=model_path)
        
        # å¯¼å…¥ç»Ÿè®¡
        self.stats = {
            'scanned_files': 0,
            'new_files': 0,
            'modified_files': 0,
            'unchanged_files': 0,
            'failed_files': 0,
            'total_chunks': 0,
            'added_chunks': 0
        }
        
        # è®°å½•å·²å¤„ç†çš„æ–‡ä»¶å“ˆå¸Œ
        self.processed_files_log = Path("./processed_files.log")
        
        print(f"âœ… å¯¼å…¥å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def load_vector_db(self, persist_directory: str, collection_name: str = "contract_law_collection"):
        """
        åŠ è½½å‘é‡æ•°æ®åº“
        """
        print(f"\nğŸ“‚ åŠ è½½å‘é‡æ•°æ®åº“...")
        print(f"   è·¯å¾„: {persist_directory}")
        print(f"   é›†åˆ: {collection_name}")
        
        if not Path(persist_directory).exists():
            print(f"âŒ é”™è¯¯: æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨: {persist_directory}")
            print("   è¯·å…ˆè¿è¡Œ create_vector_db.py åˆ›å»ºæ•°æ®åº“")
            sys.exit(1)
        
        try:
            self.vectorstore = Chroma(
                persist_directory=persist_directory,
                embedding_function=self.embeddings,
                collection_name=collection_name
            )
            
            # è·å–å½“å‰æ–‡æ¡£æ•°é‡
            current_count = self.vectorstore._collection.count()
            print(f"âœ… æ•°æ®åº“åŠ è½½æˆåŠŸ")
            print(f"   å½“å‰æ–‡æ¡£æ•°é‡: {current_count}")
            
            return self.vectorstore
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®åº“å¤±è´¥: {e}")
            sys.exit(1)
    
    def scan_directories(self, directories: List[str]) -> List[Path]:
        """
        æ‰«æç›®å½•ï¼Œè¿”å›æ‰€æœ‰JSONæ–‡ä»¶
        """
        print(f"\nğŸ” æ‰«æç›®å½•...")
        
        all_files = []
        for dir_path in directories:
            dir_path_obj = Path(dir_path)
            
            if not dir_path_obj.exists():
                print(f"âš ï¸  è­¦å‘Š: ç›®å½•ä¸å­˜åœ¨: {dir_path}")
                continue
            
            print(f"   æ‰«æ: {dir_path}")
            
            # é€’å½’æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
            for json_file in dir_path_obj.rglob("*.json"):
                all_files.append(json_file)
            
            print(f"     æ‰¾åˆ° {len([f for f in dir_path_obj.rglob('*.json')])} ä¸ªæ–‡ä»¶")
        
        self.stats['scanned_files'] = len(all_files)
        print(f"\nğŸ“Š æ€»å…±æ‰¾åˆ° {len(all_files)} ä¸ªJSONæ–‡ä»¶")
        
        return all_files
    
    def compute_file_hash(self, file_path: Path) -> str:
        """
        è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼
        ç”¨äºæ£€æµ‹æ–‡ä»¶æ˜¯å¦è¢«ä¿®æ”¹
        """
        try:
            with open(file_path, 'rb') as f:
                file_content = f.read()
            return hashlib.md5(file_content).hexdigest()
        except Exception as e:
            print(f"âŒ è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return ""
    
    def load_processed_files(self) -> Dict[str, Dict[str, Any]]:
        """
        åŠ è½½å·²å¤„ç†æ–‡ä»¶çš„è®°å½•
        è¿”å›: {æ–‡ä»¶è·¯å¾„: {å“ˆå¸Œå€¼, å¤„ç†æ—¶é—´, æ–‡æ¡£æ•°}}
        """
        processed_files = {}
        
        if self.processed_files_log.exists():
            try:
                with open(self.processed_files_log, 'r', encoding='utf-8') as f:
                    processed_files = json.load(f)
                print(f"ğŸ“– åŠ è½½äº† {len(processed_files)} ä¸ªå·²å¤„ç†æ–‡ä»¶è®°å½•")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½å¤„ç†è®°å½•å¤±è´¥: {e}")
        
        return processed_files
    
    def save_processed_files(self, processed_files: Dict[str, Dict[str, Any]]):
        """
        ä¿å­˜å·²å¤„ç†æ–‡ä»¶è®°å½•
        """
        try:
            with open(self.processed_files_log, 'w', encoding='utf-8') as f:
                json.dump(processed_files, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜å¤„ç†è®°å½•å¤±è´¥: {e}")
    
    def is_file_already_processed(self, file_path: Path, current_hash: str, 
                                 processed_files: Dict[str, Dict[str, Any]]) -> Tuple[bool, bool]:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†è¿‡
        è¿”å›: (æ˜¯å¦å·²å¤„ç†, æ˜¯å¦éœ€è¦æ›´æ–°)
        """
        file_str = str(file_path)
        
        if file_str not in processed_files:
            return False, False  # æ–°æ–‡ä»¶
        
        old_hash = processed_files[file_str].get('hash', '')
        
        if old_hash == current_hash:
            return True, False  # å·²å¤„ç†ï¼Œæ— éœ€æ›´æ–°
        else:
            return True, True  # å·²å¤„ç†ï¼Œä½†æ–‡ä»¶å·²ä¿®æ”¹ï¼Œéœ€è¦æ›´æ–°
    
    def load_documents_from_json(self, json_path: Path) -> List[Document]:
        """
        ä»JSONæ–‡ä»¶åŠ è½½æ–‡æ¡£å—
        """
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                chunks_data = json.load(f)
            
            documents = []
            for i, chunk in enumerate(chunks_data):
                text = chunk.get('text', '')
                if not text:
                    continue
                
                metadata = chunk.get('metadata', {})
                
                # æ·»åŠ æ–‡ä»¶ä¿¡æ¯
                metadata.update({
                    'source_file': str(json_path),
                    'file_name': json_path.name,
                    'chunk_index': i,
                    'total_chunks': len(chunks_data),
                    'import_time': datetime.now().isoformat()
                })
                
                doc = Document(page_content=text, metadata=metadata)
                documents.append(doc)
            
            return documents
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {json_path}: {e}")
            return []
    
    def remove_old_documents(self, file_path: Path):
        """
        åˆ é™¤æŒ‡å®šæ–‡ä»¶å¯¹åº”çš„æ—§æ–‡æ¡£
        é€šè¿‡metadataä¸­çš„source_fileå­—æ®µæŸ¥æ‰¾
        """
        try:
            file_str = str(file_path)
            
            # é€šè¿‡metadataè¿‡æ»¤æŸ¥æ‰¾æ–‡æ¡£
            # æ³¨æ„ï¼šChromaçš„deleteæ“ä½œéœ€è¦whereè¿‡æ»¤å™¨
            results = self.vectorstore.get(where={"source_file": file_str})
            
            if results and len(results['ids']) > 0:
                doc_count = len(results['ids'])
                print(f"   åˆ é™¤ {doc_count} ä¸ªæ—§æ–‡æ¡£")
                
                # åˆ é™¤æ–‡æ¡£
                self.vectorstore.delete(ids=results['ids'])
                
                return doc_count
            else:
                return 0
                
        except Exception as e:
            print(f"âš ï¸  åˆ é™¤æ—§æ–‡æ¡£å¤±è´¥ {file_path}: {e}")
            return 0
    
    def import_file(self, file_path: Path, processed_files: Dict[str, Dict[str, Any]], 
                   delete_after_import: bool = True) -> bool:
        """
        å¯¼å…¥å•ä¸ªæ–‡ä»¶åˆ°å‘é‡æ•°æ®åº“
        """
        print(f"\nğŸ“„ å¤„ç†: {file_path.name}")
        
        # 1. è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        current_hash = self.compute_file_hash(file_path)
        if not current_hash:
            self.stats['failed_files'] += 1
            return False
        
        # 2. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†
        is_processed, needs_update = self.is_file_already_processed(
            file_path, current_hash, processed_files
        )
        
        if is_processed and not needs_update:
            print(f"   â­ï¸  æ–‡ä»¶æœªä¿®æ”¹ï¼Œè·³è¿‡")
            self.stats['unchanged_files'] += 1
            return True
        
        # 3. åŠ è½½æ–‡æ¡£
        documents = self.load_documents_from_json(file_path)
        if not documents:
            print(f"   âš ï¸  æ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯")
            self.stats['failed_files'] += 1
            return False
        
        # 4. å¦‚æœæ˜¯æ›´æ–°ï¼Œå…ˆåˆ é™¤æ—§æ–‡æ¡£
        if needs_update:
            removed_count = self.remove_old_documents(file_path)
            print(f"   ğŸ”„ æ–‡ä»¶å·²ä¿®æ”¹ï¼Œåˆ é™¤ {removed_count} ä¸ªæ—§æ–‡æ¡£")
            self.stats['modified_files'] += 1
        else:
            print(f"   âœ¨ æ–°æ–‡ä»¶ï¼Œå‡†å¤‡å¯¼å…¥")
            self.stats['new_files'] += 1
        
        # 5. å¯¼å…¥æ–°æ–‡æ¡£
        try:
            print(f"   æ­£åœ¨å¯¼å…¥ {len(documents)} ä¸ªæ–‡æ¡£å—...")
            
            # åˆ†æ‰¹å¯¼å…¥ä»¥é¿å…å†…å­˜é—®é¢˜
            batch_size = 50
            for i in range(0, len(documents), batch_size):
                batch_docs = documents[i:i + batch_size]
                
                # æå–æ–‡æœ¬å’Œå…ƒæ•°æ®
                texts = [doc.page_content for doc in batch_docs]
                metadatas = [doc.metadata for doc in batch_docs]
                
                # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
                self.vectorstore.add_texts(texts=texts, metadatas=metadatas)
                
                progress = min(i + batch_size, len(documents)) / len(documents) * 100
                if len(documents) > batch_size:
                    print(f"     ğŸ”„ è¿›åº¦: {progress:.1f}%")
            
            # 6. æ›´æ–°å¤„ç†è®°å½•
            processed_files[str(file_path)] = {
                'hash': current_hash,
                'processed_at': datetime.now().isoformat(),
                'document_count': len(documents),
                'file_size': os.path.getsize(file_path)
            }
            
            # 7. åˆ é™¤æˆ–å¤‡ä»½æºæ–‡ä»¶
            # if delete_after_import:
            #     self._backup_or_delete_file(file_path)
            
            # 8. æ›´æ–°ç»Ÿè®¡
            self.stats['total_chunks'] += len(documents)
            self.stats['added_chunks'] += len(documents)
            
            print(f"   âœ… å¯¼å…¥æˆåŠŸ ({len(documents)} ä¸ªæ–‡æ¡£å—)")
            return True
            
        except Exception as e:
            print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
            self.stats['failed_files'] += 1
            return False
    
    def _backup_or_delete_file(self, file_path: Path):
        """
        å¤‡ä»½æˆ–åˆ é™¤æºæ–‡ä»¶
        """
        try:
            # # åˆ›å»ºå¤‡ä»½ç›®å½•
            # backup_dir = Path("./backup_imported_files")
            # backup_dir.mkdir(exist_ok=True)
            
            # # å¤‡ä»½æ–‡ä»¶ï¼ˆä¿ç•™ç›®å½•ç»“æ„ï¼‰
            # relative_path = file_path.relative_to(file_path.parent.parent.parent)
            # backup_path = backup_dir / relative_path
            
            # # ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
            # backup_path.parent.mkdir(parents=True, exist_ok=True)
            
            # # ç§»åŠ¨æ–‡ä»¶
            # shutil.move(str(file_path), str(backup_path))
            # print(f"   ğŸ“¦ æ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_path}")
            
            # å¯é€‰ï¼šåˆ é™¤ç©ºç›®å½•
            try:
                if file_path.parent.exists() and not any(file_path.parent.iterdir()):
                    file_path.parent.rmdir()
            except:
                pass
                
        except Exception as e:
            print(f"âš ï¸  æ–‡ä»¶å¤‡ä»½å¤±è´¥: {e}")
            # å¦‚æœå¤‡ä»½å¤±è´¥ï¼Œè‡³å°‘åˆ é™¤æºæ–‡ä»¶
            try:
                file_path.unlink()
                print(f"   ğŸ—‘ï¸  æºæ–‡ä»¶å·²åˆ é™¤")
            except:
                print(f"   âš ï¸  æ— æ³•åˆ é™¤æºæ–‡ä»¶")
    
    def import_directories(self, directories: List[str], delete_after_import: bool = True):
        """
        å¯¼å…¥æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
        """
        print(f"\nğŸš€ å¼€å§‹æ™ºèƒ½å¯¼å…¥...")
        
        # 1. æ‰«ææ–‡ä»¶
        all_files = self.scan_directories(directories)
        if not all_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•JSONæ–‡ä»¶")
            return
        
        # 2. åŠ è½½å¤„ç†è®°å½•
        processed_files = self.load_processed_files()
        
        # 3. å¯¼å…¥æ¯ä¸ªæ–‡ä»¶
        success_count = 0
        for i, file_path in enumerate(all_files):
            print(f"\n[{i+1}/{len(all_files)}] ", end="")
            
            success = self.import_file(
                file_path, 
                processed_files, 
                delete_after_import
            )
            
            if success:
                success_count += 1
        
        # 4. ä¿å­˜å¤„ç†è®°å½•
        self.save_processed_files(processed_files)
        
        # 5. æŒä¹…åŒ–å‘é‡æ•°æ®åº“
        print(f"\nğŸ’¾ ä¿å­˜å‘é‡æ•°æ®åº“...")
        self.vectorstore.persist()
        print(f"âœ… æ•°æ®åº“å·²ä¿å­˜")
        
        # 6. æ‰“å°ç»Ÿè®¡
        self._print_statistics()
        
        return success_count
    
    def _print_statistics(self):
        """æ‰“å°å¯¼å…¥ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 70)
        print("ğŸ“Š æ™ºèƒ½å¯¼å…¥ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 70)
        
        print(f"\nğŸ“ æ–‡ä»¶ç»Ÿè®¡:")
        print(f"   æ‰«ææ–‡ä»¶: {self.stats['scanned_files']}")
        print(f"   â”œâ”€ æ–°æ–‡ä»¶: {self.stats['new_files']}")
        print(f"   â”œâ”€ ä¿®æ”¹æ–‡ä»¶: {self.stats['modified_files']}")
        print(f"   â”œâ”€ æœªä¿®æ”¹æ–‡ä»¶: {self.stats['unchanged_files']}")
        print(f"   â””â”€ å¤±è´¥æ–‡ä»¶: {self.stats['failed_files']}")
        
        print(f"\nğŸ“„ æ–‡æ¡£å—ç»Ÿè®¡:")
        print(f"   æ€»æ–‡æ¡£å—: {self.stats['total_chunks']}")
        print(f"   æ–°å¢æ–‡æ¡£å—: {self.stats['added_chunks']}")
        
        if self.stats['scanned_files'] > 0:
            success_rate = (self.stats['new_files'] + self.stats['modified_files']) / self.stats['scanned_files'] * 100
            print(f"\nğŸ“ˆ å¤„ç†æˆåŠŸç‡: {success_rate:.1f}%")
    
    def test_retrieval(self, test_queries: List[str] = None):
        """
        æµ‹è¯•æ£€ç´¢åŠŸèƒ½
        """
        if test_queries is None:
            test_queries = [
                "è´·æ¬¾äººè¿çº¦æœ‰ä»€ä¹ˆåæœï¼Ÿ",
                "å€Ÿæ¬¾äººè¿çº¦æœ‰ä»€ä¹ˆåæœï¼Ÿ",
            ]
        
        print(f"\nğŸ§ª æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
        
        for query in test_queries:
            print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
            try:
                results = self.vectorstore.similarity_search(query, k=2)
                
                if results:
                    print(f"   æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ:")
                    for i, doc in enumerate(results):
                        source = doc.metadata.get('source_file', 'æœªçŸ¥')
                        clause = doc.metadata.get('clause_header', 'æœªçŸ¥æ¡æ¬¾')
                        
                        # ç®€ç•¥æ˜¾ç¤º
                        file_name = Path(source).name if source != 'æœªçŸ¥' else 'æœªçŸ¥'
                        preview = doc.page_content[:60] + "..." if len(doc.page_content) > 60 else doc.page_content
                        print(f"     {i+1}. [{file_name}] {clause}: {preview}")
                else:
                    print(f"   æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
                    
            except Exception as e:
                print(f"   æ£€ç´¢å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°ï¼šæ™ºèƒ½å¢é‡å¯¼å…¥æ–‡æ¡£"""
    print("=" * 70)
    print("ğŸ“š æ™ºèƒ½æ–‡æ¡£å¯¼å…¥å·¥å…· v2.0")
    print("åŠŸèƒ½ï¼šå¢é‡å¯¼å…¥JSONæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“ï¼Œé¿å…é‡å¤")
    print("=" * 70)
    
    # ==================== é…ç½®å‚æ•° ====================
    LOCAL_MODEL_PATH = "/root/models/text2vec-large-chinese"
    PERSIST_DIR = "../../knowledge_base"
    COLLECTION_NAME = "contract_law_collection"
    
    # è¦æ‰«æçš„ç›®å½•åˆ—è¡¨
    SCAN_DIRECTORIES = [
        "../../docs/chunks/law_chunks",
        "../../docs/chunks/contract_chunks"
    ]
    
    # æ˜¯å¦å¯¼å…¥ååˆ é™¤æºæ–‡ä»¶
    DELETE_AFTER_IMPORT = True
    
    # ==================== æ‰§è¡Œæµç¨‹ ====================
    print(f"\nğŸ“ é…ç½®ä¿¡æ¯:")
    print(f"   æ¨¡å‹è·¯å¾„: {LOCAL_MODEL_PATH}")
    print(f"   æ•°æ®åº“ä½ç½®: {PERSIST_DIR}")
    print(f"   é›†åˆåç§°: {COLLECTION_NAME}")
    print(f"   æ‰«æç›®å½•: {SCAN_DIRECTORIES}")
    print(f"   å¯¼å…¥ååˆ é™¤æºæ–‡ä»¶: {'æ˜¯' if DELETE_AFTER_IMPORT else 'å¦'}")
    
    # 1. åˆå§‹åŒ–å¯¼å…¥å™¨
    print(f"\n1. ğŸš€ åˆå§‹åŒ–æ™ºèƒ½å¯¼å…¥å™¨...")
    importer = SmartDocumentImporter(model_path=LOCAL_MODEL_PATH)
    
    # 2. åŠ è½½å‘é‡æ•°æ®åº“
    print(f"\n2. ğŸ“‚ åŠ è½½å‘é‡æ•°æ®åº“...")
    vectorstore = importer.load_vector_db(PERSIST_DIR, COLLECTION_NAME)
    
    # è·å–å½“å‰æ–‡æ¡£æ•°é‡
    current_count = vectorstore._collection.count()
    print(f"   å½“å‰æ–‡æ¡£æ•°é‡: {current_count}")
    
    # 3. ç¡®è®¤æ“ä½œ
    print(f"\nâš ï¸  ç¡®è®¤æ“ä½œ:")
    print(f"   å°†æ‰«æ {len(SCAN_DIRECTORIES)} ä¸ªç›®å½•")
    print(f"   æ•°æ®åº“å½“å‰æœ‰ {current_count} ä¸ªæ–‡æ¡£")
    
    confirm = input("   æ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ")
    if confirm.lower() != 'y':
        print("æ“ä½œå–æ¶ˆ")
        sys.exit(0)
    
    # 4. å¯¼å…¥æ–‡æ¡£
    print(f"\n3. ğŸš€ å¼€å§‹æ™ºèƒ½å¯¼å…¥...")
    success_count = importer.import_directories(
        directories=SCAN_DIRECTORIES,
        delete_after_import=DELETE_AFTER_IMPORT
    )
    
    # 5. æµ‹è¯•æ£€ç´¢
    print(f"\n4. ğŸ§ª æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
    new_count = vectorstore._collection.count()
    print(f"   æ›´æ–°åæ–‡æ¡£æ€»æ•°: {new_count}")
    print(f"   æœ¬æ¬¡æ–°å¢æ–‡æ¡£: {new_count - current_count}")
    
    if new_count > current_count:
        importer.test_retrieval()
    
    # ==================== æ€»ç»“ ====================
    print("\n" + "=" * 70)
    print("ğŸ‰ æ™ºèƒ½å¯¼å…¥å®Œæˆï¼")
    print("=" * 70)
    
    print(f"\nğŸ’¡ åŠŸèƒ½ç‰¹ç‚¹:")
    print(f"   â€¢ æ–‡ä»¶å“ˆå¸Œæ£€æµ‹: é¿å…é‡å¤å¯¼å…¥ç›¸åŒæ–‡ä»¶")
    print(f"   â€¢ å¢é‡æ›´æ–°: åªå¤„ç†æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡ä»¶")
    print(f"   â€¢ è®°å½•è·Ÿè¸ª: å·²å¤„ç†æ–‡ä»¶è®°å½•åœ¨ processed_files.log")
    
    print(f"\nğŸ“Š æ•°æ®åº“çŠ¶æ€:")
    print(f"   æ–‡æ¡£æ€»æ•°: {new_count}")
    print(f"   ä½ç½®: {PERSIST_DIR}")
    print(f"   ä¸‹æ¬¡è¿è¡Œ: åªä¼šå¤„ç†æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡ä»¶")
    
    print(f"\nâœ… å®Œæˆï¼")


if __name__ == "__main__":
    main()