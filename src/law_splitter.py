import os
import re
from typing import List, Dict, Any
from pathlib import Path
from docx import Document
import json

class UniversalLegalTextSplitter:
    """é€šç”¨æ³•å¾‹æ–‡æ¡£æ–‡æœ¬åˆ†å‰²å™¨ï¼Œé€‚ç”¨äºå„ç§æ³•å¾‹æ–‡æ¡£æ ¼å¼"""
    
    def __init__(self, chunk_size=800, chunk_overlap=100, min_chunk_length=20):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_length = min_chunk_length
        
        # é€šç”¨æ³•å¾‹æ¡æ¬¾è¯†åˆ«æ¨¡å¼
        self.clause_patterns = [
            # ä¸­æ–‡æ•°å­—æ¡æ¬¾: ç¬¬ä¸€æ¡ã€ç¬¬ä¸€ç™¾äºŒåä¸‰æ¡
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡é›¶]+æ¡[^\n]*',
            # é˜¿æ‹‰ä¼¯æ•°å­—æ¡æ¬¾: ç¬¬1æ¡ã€ç¬¬123æ¡
            r'ç¬¬\d+æ¡[^\n]*',
            # ç« èŠ‚æ ‡é¢˜: ç¬¬ä¸€ç« ã€ç¬¬åç« 
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ç« [^\n]*',
            # å°èŠ‚æ ‡é¢˜: ç¬¬ä¸€èŠ‚ã€ç¬¬äºŒèŠ‚
            r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+èŠ‚[^\n]*',
            # å¸¦ç‚¹ç¼–å·: 1.1ã€2.3.1
            r'\d+\.\d+(\.\d+)*[^\n]*',
            # ä¸­æ–‡åºå·: ä¸€ã€äºŒã€
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€.][^\n]*',
            # æ‹¬å·ä¸­æ–‡åºå·: (ä¸€)ã€(äºŒ)
            r'[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ï¼‰)][^\n]*',
            # æ‹¬å·é˜¿æ‹‰ä¼¯åºå·: (1)ã€(2)
            r'[ï¼ˆ(]\d+[ï¼‰)][^\n]*',
            # å¸¦åœˆæ•°å­—: â‘ ã€â‘¡
            r'[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©][^\n]*',
            # å­—æ¯ç¼–å·: a)ã€b) æˆ– Aã€B
            r'[a-zA-Z][)ã€.][^\n]*',
        ]
    
    def split_by_clauses(self, text: str) -> List[str]:
        """
        æŒ‰æ³•å¾‹æ¡æ¬¾åˆ†å‰²æ–‡æœ¬ï¼Œä¿æŒæ¡æ¬¾å®Œæ•´æ€§
        
        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            
        Returns:
            åˆ†å‰²åçš„æ¡æ¬¾åˆ—è¡¨
        """
        if not text or not text.strip():
            return []
        
        # ç»„åˆæ‰€æœ‰æ¨¡å¼
        combined_pattern = '(' + '|'.join(self.clause_patterns) + ')'
        
        # æŸ¥æ‰¾æ‰€æœ‰æ¡æ¬¾å¼€å§‹ä½ç½®
        clause_starts = []
        for pattern in self.clause_patterns:
            for match in re.finditer(pattern, text):
                clause_starts.append((match.start(), match.group()))
        
        # å»é‡å¹¶æ’åº
        clause_starts = sorted(set(clause_starts), key=lambda x: x[0])
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ¡æ¬¾ï¼Œå°†æ•´ä¸ªæ–‡æœ¬ä½œä¸ºä¸€ä¸ªå—
        if not clause_starts:
            return [text.strip()] if len(text.strip()) >= self.min_chunk_length else []
        
        chunks = []
        
        # å¤„ç†ç¬¬ä¸€ä¸ªæ¡æ¬¾ä¹‹å‰çš„å†…å®¹
        if clause_starts[0][0] > 0:
            pre_text = text[:clause_starts[0][0]].strip()
            if pre_text and len(pre_text) >= self.min_chunk_length:
                chunks.append(pre_text)
        
        # æŒ‰æ¡æ¬¾åˆ†å‰²
        for i in range(len(clause_starts)):
            start_pos, clause_header = clause_starts[i]
            
            # ç¡®å®šç»“æŸä½ç½®
            if i + 1 < len(clause_starts):
                end_pos = clause_starts[i+1][0]
            else:
                end_pos = len(text)
            
            # æå–æ¡æ¬¾æ–‡æœ¬
            clause_text = text[start_pos:end_pos].strip()
            
            if clause_text and len(clause_text) >= self.min_chunk_length:
                chunks.append(clause_text)
        
        return chunks
    
    def detect_clause_type(self, text: str) -> str:
        """
        æ£€æµ‹æ–‡æœ¬ä¸­çš„æ¡æ¬¾ç±»å‹
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            æ¡æ¬¾ç±»å‹æè¿°
        """
        for i, pattern in enumerate(self.clause_patterns):
            if re.match(pattern, text):
                types = [
                    "ä¸­æ–‡æ¡æ¬¾", "æ•°å­—æ¡æ¬¾", "ç« èŠ‚æ ‡é¢˜", "å°èŠ‚æ ‡é¢˜", 
                    "ç¼–å·æ¡æ¬¾", "ä¸­æ–‡åºå·", "æ‹¬å·ä¸­æ–‡", "æ‹¬å·æ•°å­—",
                    "å¸¦åœˆæ•°å­—", "å­—æ¯ç¼–å·"
                ]
                return types[i] if i < len(types) else "æœªçŸ¥æ¡æ¬¾"
        return "æ™®é€šæ–‡æœ¬"

class UniversalLegalDocumentProcessor:
    """é€šç”¨æ³•å¾‹æ–‡æ¡£å¤„ç†å™¨ï¼Œé€‚ç”¨äºå„ç§æ³•å¾‹æ–‡æ¡£æ ¼å¼"""
    
    def __init__(self, laws_dir: str = "../docs/laws", chunk_size=800, min_chunk_length=20):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            laws_dir: æ³•å¾‹æ–‡æ¡£ç›®å½•è·¯å¾„
            chunk_size: å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            min_chunk_length: æœ€å°å—é•¿åº¦
        """
        self.laws_dir = Path(laws_dir)
        self.docs = []
        self.splitter = UniversalLegalTextSplitter(
            chunk_size=chunk_size, 
            min_chunk_length=min_chunk_length
        )
    
    def extract_docx_text(self, file_path: Path) -> Dict[str, Any]:
        """ä»docxæ–‡ä»¶ä¸­æå–æ–‡æœ¬"""
        try:
            doc = Document(file_path)
            paragraphs = [p.text.strip() for p in doc.paragraphs if p.text.strip()]
            full_text = '\n\n'.join(paragraphs)
            
            return {
                'file_name': file_path.name,
                'file_path': str(file_path),
                'full_text': full_text,
                'total_paragraphs': len(paragraphs),
                'extraction_success': True
            }
        except Exception as e:
            print(f"æå–æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            return {
                'file_name': file_path.name,
                'file_path': str(file_path),
                'full_text': '',
                'error': str(e),
                'extraction_success': False
            }
    
    def load_all_documents(self) -> List[Dict[str, Any]]:
        """åŠ è½½ç›®å½•ä¸­çš„æ‰€æœ‰docxæ–‡æ¡£"""
        if not self.laws_dir.exists():
            print(f"ç›®å½•ä¸å­˜åœ¨: {self.laws_dir}")
            return []
        
        # æ”¯æŒå¤šç§æ–‡æ¡£æ‰©å±•å
        extensions = ["*.docx", "*.doc"]
        doc_files = []
        for ext in extensions:
            doc_files.extend(list(self.laws_dir.glob(ext)))
        
        print(f"æ‰¾åˆ° {len(doc_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        
        all_docs = []
        for file_path in doc_files:
            print(f"æ­£åœ¨å¤„ç†: {file_path.name}")
            doc_data = self.extract_docx_text(file_path)
            
            if doc_data['extraction_success']:
                all_docs.append(doc_data)
                print(f"  âœ“ æˆåŠŸæå–ï¼Œ{len(doc_data['full_text'])} å­—ç¬¦")
            else:
                print(f"  âœ— æå–å¤±è´¥: {doc_data.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        self.docs = all_docs
        return all_docs
    
    def process_documents(self) -> List[Dict[str, Any]]:
        """å¤„ç†å¹¶åˆ†å‰²æ‰€æœ‰æ–‡æ¡£"""
        if not self.docs:
            self.load_all_documents()
        
        all_chunks = []
        
        for doc in self.docs:
            if not doc['extraction_success']:
                continue
            
            print(f"åˆ†å‰²æ–‡æ¡£: {doc['file_name']}")
            
            # ä½¿ç”¨æ¡æ¬¾åˆ†å‰²
            chunks = self.splitter.split_by_clauses(doc['full_text'])
            
            print(f"  åˆ†å‰²ä¸º {len(chunks)} ä¸ªå—")
            
            # æ·»åŠ å…ƒæ•°æ®
            for i, chunk in enumerate(chunks):
                # æ£€æµ‹æ¡æ¬¾ç±»å‹
                clause_type = self.splitter.detect_clause_type(chunk)
                
                # æå–æ¡æ¬¾æ ‡é¢˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                clause_header = "æ™®é€šæ–‡æœ¬"
                if clause_type != "æ™®é€šæ–‡æœ¬":
                    # æå–å‰50ä¸ªå­—ç¬¦ä½œä¸ºæ¡æ¬¾æ ‡é¢˜
                    clause_header = chunk[:50].split('\n')[0].strip()
                    if len(clause_header) > 30:
                        clause_header = clause_header[:30] + "..."
                
                all_chunks.append({
                    'text': chunk,
                    'metadata': {
                        'source': doc['file_name'],
                        'file_path': doc['file_path'],
                        'chunk_index': i,
                        'total_chunks_in_doc': len(chunks),
                        'chunk_size': len(chunk),
                        'clause_type': clause_type,
                        'clause_header': clause_header,
                        'chunk_preview': chunk[:100].replace('\n', ' ') + ("..." if len(chunk) > 100 else "")
                    }
                })
        
        return all_chunks
    
    def save_results(self, chunks: List[Dict[str, Any]], output_dir: str = "./law_chunks"):
        """ä¿å­˜åˆ†å‰²ç»“æœ - æŒ‰æ–‡æ¡£åç§°åˆ›å»ºæ–‡ä»¶å¤¹"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # æŒ‰æ–‡æ¡£åˆ†ç»„
        docs_groups = {}
        for chunk in chunks:
            source = chunk['metadata']['source']
            if source not in docs_groups:
                docs_groups[source] = []
            docs_groups[source].append(chunk)
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºæ–‡ä»¶å¤¹å¹¶ä¿å­˜
        doc_folders = {}
        for source, doc_chunks in docs_groups.items():
            # ç§»é™¤æ–‡ä»¶æ‰©å±•åï¼Œä½¿ç”¨æ–‡æ¡£åç§°ä½œä¸ºæ–‡ä»¶å¤¹å
            doc_name = Path(source).stem
            doc_folder = output_path / doc_name
            doc_folder.mkdir(exist_ok=True)
            doc_folders[source] = str(doc_folder)
            
            # æŒ‰å—ç´¢å¼•æ’åº
            doc_chunks.sort(key=lambda x: x['metadata']['chunk_index'])
            
            # ä¿å­˜è¯¥æ–‡æ¡£çš„å—åˆ°è‡ªå·±çš„æ–‡ä»¶å¤¹
            doc_json_path = doc_folder / "chunks.json"
            with open(doc_json_path, 'w', encoding='utf-8') as f:
                json.dump(doc_chunks, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜è¯¥æ–‡æ¡£çš„ç»Ÿè®¡ä¿¡æ¯
            doc_stats_path = doc_folder / "statistics.txt"
            with open(doc_stats_path, 'w', encoding='utf-8') as f:
                self._write_document_statistics(f, source, doc_chunks)
            
            print(f"  âœ“ æ–‡æ¡£ '{source}' çš„åˆ†å‰²ç»“æœå·²ä¿å­˜åˆ°: {doc_folder}")
        
        # ä¿å­˜æ€»çš„ç»Ÿè®¡ä¿¡æ¯
        stats_path = output_path / "law_split_statistics.txt"
        with open(stats_path, 'w', encoding='utf-8') as f:
            self._write_overall_statistics(f, chunks, docs_groups, doc_folders)
        
        print(f"âœ“ æ€»ä½“ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_path}")
        
        return str(output_path)
    
    def _write_document_statistics(self, f, source: str, doc_chunks: List[Dict[str, Any]]):
        """å†™å…¥å•ä¸ªæ–‡æ¡£çš„ç»Ÿè®¡ä¿¡æ¯"""
        f.write("=" * 80 + "\n")
        f.write(f"æ–‡æ¡£åˆ†å‰²ç»Ÿè®¡: {source}\n")
        f.write("=" * 80 + "\n\n")
        
        f.write(f"ğŸ“Š æ–‡æ¡£ç»Ÿè®¡\n")
        f.write(f"  æ–‡æ¡£åç§°: {source}\n")
        f.write(f"  æ€»å—æ•°: {len(doc_chunks)}\n")
        
        if doc_chunks:
            avg_chunk_size = sum(len(c['text']) for c in doc_chunks) / len(doc_chunks)
            max_chunk_size = max(len(c['text']) for c in doc_chunks)
            min_chunk_size = min(len(c['text']) for c in doc_chunks)
            
            f.write(f"  å¹³å‡å—å¤§å°: {avg_chunk_size:.0f} å­—ç¬¦\n")
            f.write(f"  æœ€å¤§å—å¤§å°: {max_chunk_size} å­—ç¬¦\n")
            f.write(f"  æœ€å°å—å¤§å°: {min_chunk_size} å­—ç¬¦\n")
            
            # æŒ‰æ¡æ¬¾ç±»å‹ç»Ÿè®¡
            clause_types = {}
            for chunk in doc_chunks:
                clause_type = chunk['metadata']['clause_type']
                clause_types[clause_type] = clause_types.get(clause_type, 0) + 1
            
            f.write(f"\nğŸ“‹ æ¡æ¬¾ç±»å‹ç»Ÿè®¡\n")
            for clause_type, count in sorted(clause_types.items(), key=lambda x: x[1], reverse=True):
                percentage = count / len(doc_chunks) * 100
                f.write(f"  {clause_type}: {count} ä¸ªå— ({percentage:.1f}%)\n")
            
            # æ£€æŸ¥è¿ç»­æ€§
            indices = [c['metadata']['chunk_index'] for c in doc_chunks]
            if indices:
                min_idx = min(indices)
                max_idx = max(indices)
                
                # æ‰¾å‡ºç¼ºå¤±çš„ç´¢å¼•
                all_indices = set(range(min_idx, max_idx + 1))
                present_indices = set(indices)
                missing_indices = sorted(all_indices - present_indices)
                
                if missing_indices:
                    f.write(f"\nâš  è¿ç»­æ€§æ£€æŸ¥\n")
                    f.write(f"  ç¼ºå¤±å—ç´¢å¼•: {missing_indices}\n")
                    f.write(f"  ç¼ºå¤±å—æ•°é‡: {len(missing_indices)}\n")
            
            # åˆ—å‡ºæ‰€æœ‰å—
            f.write(f"\n{'â”€' * 60}\n")
            f.write(f"è¯¦ç»†å—åˆ—è¡¨ (å…±{len(doc_chunks)}ä¸ªå—):\n")
            f.write(f"{'â”€' * 60}\n")
            
            for chunk in doc_chunks:
                idx = chunk['metadata']['chunk_index']
                clause_type = chunk['metadata']['clause_type']
                clause_header = chunk['metadata']['clause_header']
                size = chunk['metadata']['chunk_size']
                
                # æå–é¢„è§ˆæ–‡æœ¬
                preview = chunk['text'][:80].replace('\n', ' ')
                if len(chunk['text']) > 80:
                    preview += "..."
                
                f.write(f"å— {idx:3d}: [{clause_type}] {clause_header} ({size:4d}å­—ç¬¦)\n")
                f.write(f"     é¢„è§ˆ: {preview}\n")
    
    def _write_overall_statistics(self, f, chunks: List[Dict[str, Any]], 
                                 docs_groups: Dict[str, List], doc_folders: Dict[str, str]):
        """å†™å…¥æ€»ä½“ç»Ÿè®¡ä¿¡æ¯"""
        f.write("=" * 80 + "\n")
        f.write("æ³•å¾‹æ–‡æ¡£åˆ†å‰²æ€»ä½“ç»Ÿè®¡æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        
        # æ€»ä½“ç»Ÿè®¡
        f.write(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡\n")
        f.write(f"  æ–‡æ¡£æ€»æ•°: {len(docs_groups)}\n")
        f.write(f"  æ€»å—æ•°: {len(chunks)}\n")
        
        if chunks:
            avg_chunk_size = sum(len(c['text']) for c in chunks) / len(chunks)
            f.write(f"  å¹³å‡å—å¤§å°: {avg_chunk_size:.0f} å­—ç¬¦\n")
        
        # å„æ–‡æ¡£ç»Ÿè®¡æ‘˜è¦
        f.write(f"\nğŸ“ å„æ–‡æ¡£ç»Ÿè®¡æ‘˜è¦\n")
        for source, doc_chunks in docs_groups.items():
            doc_name = Path(source).stem
            folder_path = doc_folders[source]
            avg_size = sum(len(c['text']) for c in doc_chunks) / len(doc_chunks) if doc_chunks else 0
            
            f.write(f"\n  ğŸ“„ æ–‡æ¡£: {source}\n")
            f.write(f"    å—æ•°: {len(doc_chunks)}\n")
            f.write(f"    å¹³å‡å—å¤§å°: {avg_size:.0f} å­—ç¬¦\n")
            f.write(f"    ä¿å­˜ä½ç½®: {folder_path}/\n")
            
            # æ£€æŸ¥è¿ç»­æ€§
            indices = [c['metadata']['chunk_index'] for c in doc_chunks]
            if indices:
                min_idx = min(indices)
                max_idx = max(indices)
                expected_count = max_idx - min_idx + 1
                actual_count = len(doc_chunks)
                
                if expected_count != actual_count:
                    f.write(f"    âš  è¿ç»­æ€§è­¦å‘Š: åº”æœ‰{expected_count}ä¸ªå—ï¼Œå®é™…{actual_count}ä¸ªå—\n")
        
        # æ¡æ¬¾ç±»å‹åˆ†å¸ƒ
        f.write(f"\nğŸ“‹ æ€»ä½“æ¡æ¬¾ç±»å‹åˆ†å¸ƒ\n")
        clause_types = {}
        for chunk in chunks:
            clause_type = chunk['metadata']['clause_type']
            clause_types[clause_type] = clause_types.get(clause_type, 0) + 1
        
        for clause_type, count in sorted(clause_types.items(), key=lambda x: x[1], reverse=True):
            percentage = count / len(chunks) * 100
            f.write(f"  {clause_type}: {count} ä¸ªå— ({percentage:.1f}%)\n")
        
        # ä¿å­˜ä½ç½®ä¿¡æ¯
        f.write(f"\nğŸ’¾ æ–‡ä»¶ä¿å­˜ä½ç½®\n")
        f.write(f"  æ€»ç»Ÿè®¡æ–‡ä»¶: {Path.cwd() / 'law_chunks' / 'split_statistics.txt'}\n")
        f.write(f"  å„æ–‡æ¡£åˆ†å‰²ç»“æœ:\n")
        for source, folder_path in doc_folders.items():
            f.write(f"    â€¢ {source}: {folder_path}/\n")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("é€šç”¨æ³•å¾‹æ–‡æ¡£åˆ†å‰²å™¨")
    print("=" * 80)
    
    # é…ç½®å‚æ•°
    import argparse
    
    parser = argparse.ArgumentParser(description="é€šç”¨æ³•å¾‹æ–‡æ¡£åˆ†å‰²å™¨")
    parser.add_argument("--input-dir", default="../docs/laws_cleaned", help="è¾“å…¥æ–‡æ¡£ç›®å½•")
    parser.add_argument("--output-dir", default="../docs/chunks/law_chunks", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--chunk-size", type=int, default=800, help="å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰")
    parser.add_argument("--min-length", type=int, default=20, help="æœ€å°å—é•¿åº¦")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = UniversalLegalDocumentProcessor(
        laws_dir=args.input_dir,
        chunk_size=args.chunk_size,
        min_chunk_length=args.min_length
    )
    
    # 1. åŠ è½½æ–‡æ¡£
    print(f"\n[æ­¥éª¤1] ä» '{args.input_dir}' åŠ è½½æ³•å¾‹æ–‡æ¡£...")
    docs = processor.load_all_documents()
    
    if not docs:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡æ¡£ã€‚è¯·æ£€æŸ¥:")
        print(f"   1. ç›®å½•æ˜¯å¦å­˜åœ¨: {args.input_dir}")
        print(f"   2. ç›®å½•ä¸­æ˜¯å¦æœ‰.docxæˆ–.docæ–‡ä»¶")
        return
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
    
    # 2. åˆ†å‰²æ–‡æ¡£
    print(f"\n[æ­¥éª¤2] æŒ‰æ¡æ¬¾åˆ†å‰²æ–‡æ¡£...")
    chunks = processor.process_documents()
    print(f"âœ… æ€»åˆ†å‰²å—æ•°: {len(chunks)}")
    
    # 3. ä¿å­˜ç»“æœ
    print(f"\n[æ­¥éª¤3] ä¿å­˜åˆ†å‰²ç»“æœåˆ° '{args.output_dir}'...")
    print("  æŒ‰æ–‡æ¡£åç§°åˆ›å»ºæ–‡ä»¶å¤¹ï¼Œä¿å­˜åˆ†å‰²ç»“æœ:")
    output_path = processor.save_results(chunks, args.output_dir)
    
    # 4. æ˜¾ç¤ºæ€»ç»“
    print(f"\n" + "=" * 80)
    print("å¤„ç†å®Œæˆ!")
    print("=" * 80)
    
    # æ˜¾ç¤ºå¤„ç†ç»“æœ
    if chunks:
        print(f"\nğŸ“‹ å¤„ç†æ‘˜è¦:")
        print(f"  â€¢ è¾“å…¥æ–‡æ¡£: {len(docs)} ä¸ª")
        print(f"  â€¢ è¾“å‡ºå—æ•°: {len(chunks)} ä¸ª")
        print(f"  â€¢ è¾“å‡ºç›®å½•: {output_path}")
        
        # æ˜¾ç¤ºå„æ–‡æ¡£çš„è¾“å‡ºä½ç½®
        print(f"\nğŸ“ å„æ–‡æ¡£è¾“å‡ºä½ç½®:")
        import os
        for doc in docs:
            if doc['extraction_success']:
                doc_name = Path(doc['file_name']).stem
                doc_folder = Path(output_path) / doc_name
                if doc_folder.exists():
                    print(f"  â€¢ {doc['file_name']}: {doc_folder}/")

if __name__ == "__main__":
    main()